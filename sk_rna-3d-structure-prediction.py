# è¯¥ tm_score_loss å®ç°å‚è€ƒäº† [Kaggle Ribonanza æ¯”èµ›é€‰æ‰‹ Handsonlabs Software Academy] çš„å¼€æºä»£ç :https://www.kaggle.com/code/tobimichigan/rna-3d-structure-prediction-pipeline-algorithm

import torch
import torch.nn as nn
from torch.nn import functional as F
import torch.optim as optim
import random
import string
from torchtune.modules import RotaryPositionalEmbeddings
from torch.utils.data import Dataset, DataLoader,random_split
import pandas as pd
import numpy as np
from torch.nn.utils.rnn import pad_sequence
import os
from tqdm import tqdm
from sklearn.model_selection import KFold


#const
device = 'mps' if torch.backends.mps.is_available() else 'cpu' #'cuda' if torch.cuda.is_available() else 'cpu'

#æ•°æ®é¢„å¤„ç†
# RNA_PARA_FILE = "/kaggle/input/rna-llm-parameter-file/"
RNA_PARA_FILE = ""
RNA_TRANS_MODEL_FILE = f"{RNA_PARA_FILE}rna_transformer_model.pth"
RNA_RNN_MODEL_FILE = f"{RNA_PARA_FILE}rna_3d_rnn_model.pth"
RNA_REPR_CACHE_FILE = f"{RNA_PARA_FILE}string_repr_cache.pt"
RNA_PERP_CACHE_VAL_FILE = f"{RNA_PARA_FILE}string_repr_cache_val.pt"

DATAPATH="stanford-rna-3d-folding"
TRAIN_SEQ_FILE_PATH = f"{DATAPATH}/train_sequences.v2.csv"
TRAIN_LABEL_FILE_PATH = f"{DATAPATH}/train_labels.v2.csv"
VALI_SEQ_FILE_PATH = f"{DATAPATH}/validation_sequences.csv"
VALI_LABEL_FILE_PATH = f"{DATAPATH}/validation_labels.csv"
TEST_SEQ_FILE_PATH = f"{DATAPATH}/test_sequences.csv"

# æ ‡å‡†åŒ–å‚æ•°ï¼ˆæå‰è®¾ç½®å¥½ï¼‰
# x åæ ‡å‡å€¼: 168.97385026312512, æ–¹å·®: 123.40697723145699
# y åæ ‡å‡å€¼: 171.8862117561932, æ–¹å·®: 121.90471836458826
# z åæ ‡å‡å€¼: 168.37776717440175, æ–¹å·®: 126.73294381590155
COORDS_MEAN = torch.tensor([168.97, 171.89, 168.38], device=device)
COORDS_STD = torch.tensor([123.41, 121.90, 126.73], device=device)

"**************************************ç”ŸæˆRNAåºåˆ—åŸå‘é‡***********************************************"
#å‡†å¤‡æ•°æ®
def generate_random_string_with_length_range(min_length, max_length, chars=string.ascii_letters + string.digits):
  if min_length > max_length:
    return ""

  length = random.randint(min_length, max_length)
  random_string = ''.join(random.choice(chars) for _ in range(length))
  return random_string

#RNAä¸­æœ€åŸºæœ¬çš„å››ç§ç¢±åŸºä¸ºè…ºå˜Œå‘¤ï¼ˆAï¼‰ã€å°¿å˜§å•¶ï¼ˆUï¼‰ã€é¸Ÿå˜Œå‘¤ï¼ˆGï¼‰ã€èƒå˜§å•¶ï¼ˆCï¼‰
#åºåˆ—å­—ç¬¦é€šé“ä¸º6ï¼Œä¸è€ƒè™‘å…¶ä»–ä¿¡æ¯ï¼Œå­—ç¬¦å–ACGUX-
def generate_init_data(numbers=10000):
    # return [generate_random_string_with_length_range(4,10,chars=string.ascii_uppercase) for _ in range(numbers)]
    return [generate_random_string_with_length_range(4,256,chars="ACGUX-") for _ in range(numbers)]


class CharDataset(Dataset):
    def __init__(self, data, stoi):
        self.data = data
        self.stoi = stoi

    def __len__(self):
        return len(self.data)

    def encode(self, s):
        ids = [self.stoi[c] for c in s]
        return torch.tensor(ids, dtype=torch.long)

    def __getitem__(self, idx):
        s = self.data[idx]
        x = self.encode(s)
        y = x.clone() 
        return x, y


# å›ºå®š vocabï¼ˆåŒ…å« <pad>ï¼‰
VOCAB = ['<pad>'] + list("ACGUX-")
stoi = {ch: i for i, ch in enumerate(VOCAB)}
itos = {i: ch for ch, i in stoi.items()}
vocab_size = len(stoi)

#æ¨¡å‹
class TransformerBlockWithRoPE(nn.Module):
    def __init__(self, d_model, nhead, rope):
        super().__init__()
        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)
        self.rope = rope
        self.norm1 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Linear(d_model * 4, d_model)
        )
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        dim_per_head = d_model // self.attn.num_heads
        q = k = v = x.view(batch_size, seq_len, self.attn.num_heads, dim_per_head)  # è°ƒæ•´å½¢çŠ¶
        q= self.rope(q)
        k= self.rope(k)
        q = q.view(batch_size, seq_len, d_model)  # æ¢å¤å½¢çŠ¶
        k = k.view(batch_size, seq_len, d_model)  # æ¢å¤å½¢çŠ¶
        v = v.view(batch_size, seq_len, d_model)
        attn_out, _ = self.attn(q, k, v)
        x = self.norm1(x + attn_out)
        x = self.norm2(x + self.ff(x))
        return x

class SimpleCharTransformerWithRoPE(nn.Module):
    def __init__(self, vocab_size, d_model=512, nhead=2, num_layers=2):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.rope = RotaryPositionalEmbeddings(dim=d_model // nhead, max_seq_len=8192)
        self.layers = nn.ModuleList([
            TransformerBlockWithRoPE(d_model, nhead, self.rope)
            for _ in range(num_layers)
        ])
        self.output = nn.Linear(d_model, vocab_size)
        self.hidden_states = []

    def forward(self, x):
        emb = self.embedding(x)  # [B, T, D]
        h = emb
        self.hidden_states = []
        for layer in self.layers:
            h = layer(h)
            self.hidden_states.append(h.detach())
        return self.output(h)
    
# -------------------------------
# 5. è®­ç»ƒä¸»æµç¨‹
# -------------------------------

def rna_str_train():
    
    data = generate_init_data(10000)
    dataset = CharDataset(data, stoi)
    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

    model = SimpleCharTransformerWithRoPE(vocab_size).to(device)
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    optimizer = optim.AdamW(model.parameters(), lr=1e-3)

    for epoch in range(10):
        model.train()
        total_loss = 0
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            logits = model(x)  # [B, T, V]
            loss = criterion(logits.view(-1, vocab_size), y.view(-1))

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

        # æ‰“å°éšè—å±‚ä¿¡æ¯
        model.eval()
        sample_x, _ = next(iter(dataloader))
        sample_x = sample_x.to(device)
        with torch.no_grad():
            logits = model(sample_x)

        # print("=== éšè—å±‚è¾“å‡º ===")
        # for layer_idx, h in enumerate(model.hidden_states):
        #     print(f"ç¬¬ {layer_idx+1} å±‚: shape = {h.shape}")
        #     for i in range(2):  # æ‰“å°å‰ä¸¤ä¸ªæ ·æœ¬çš„å‰å‡ ä¸ªtokençš„å‘é‡
        #         valid_len = (sample_x[i] != 0).sum().item()
        #         print(f"æ ·æœ¬ {i}ï¼Œæœ‰æ•ˆé•¿åº¦: {valid_len}")
        #         print(h[i, :valid_len].cpu().numpy())  # [T, D]
        # print("=" * 40)
        # # æ¯è½®æŸ¥çœ‹é¢„æµ‹æ•ˆæœ
        # model.eval()
        # sample_x, _ = next(iter(dataloader))
        # sample_x = sample_x.to(device)
        # with torch.no_grad():
        #     logits = model(sample_x)
        # preds = torch.argmax(logits, dim=-1)
        # for i in range(3):
        #     input_str = ''.join([itos[idx.item()] for idx in sample_x[i] if idx.item() != 0])
        #     output_str = ''.join([itos[idx.item()] for idx in preds[i] if idx.item() != 0])
        #     print(f"[{i}] input: {input_str}, output: {output_str}")
        # print("-" * 30)
        # è·å–æœ€åä¸€å±‚éšè—çŠ¶æ€
        last_hidden = model.hidden_states[-1]  # [B, T, D]
        # ç”¨ mask å»æ‰ padding
        mask = (sample_x != 0).unsqueeze(-1).float()  # [B, T, 1]
        masked_hidden = last_hidden * mask  # ä»…ä¿ç•™épaddingçš„å‘é‡

        # æ¯ä¸ªæ ·æœ¬æœ‰å¤šå°‘épadding token
        valid_lengths = mask.sum(dim=1)  # [B, 1]

        # æ±‚å¹³å‡ç¼–ç ï¼ˆå­—ç¬¦ä¸²æ•´ä½“è¡¨ç¤ºï¼‰
        string_repr = masked_hidden.sum(dim=1) / valid_lengths  # [B, D]

        print("å­—ç¬¦ä¸²æ•´ä½“ç¼–ç  shape:", string_repr.shape)
        print("å‰ä¸¤ä¸ªå­—ç¬¦ä¸²æ•´ä½“è¡¨ç¤ºå‘é‡:")
        print(string_repr[:2].cpu().numpy())
            # ä¿å­˜æ¨¡å‹
        torch.save(model.state_dict(),RNA_TRANS_MODEL_FILE)
        print("æ¨¡å‹å·²ä¿å­˜ä¸º rna_transformer_model.pth")

def encode_string(s, stoi):
    ids = [stoi[c] for c in s]
    return torch.tensor([ids], dtype=torch.long)  # shape [1, T]


def rna_str_eval(input_str):
    model = SimpleCharTransformerWithRoPE(vocab_size).to(device)
    model.load_state_dict(torch.load(RNA_TRANS_MODEL_FILE, map_location=device))
    model.eval()  

    # è¾“å…¥å­—ç¬¦ä¸²
    input_tensor = encode_string(input_str,stoi).to(device)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        logits = model(input_tensor)
    char_emb = model.embedding(input_tensor)  # [B, T, D]
    last_hidden = model.hidden_states[-1]  # [B, T, D]
    mask = (input_tensor != 0).unsqueeze(-1).float()
    valid_lengths = mask.sum(dim=1)  # [B, 1]
    string_repr = (last_hidden * mask).sum(dim=1) / valid_lengths  # [B, D]
    string_repr_expanded = string_repr.unsqueeze(1).expand_as(char_emb)  # [B, T, D]
    fused_char_repr = char_emb + string_repr_expanded  # [B, T, D]
    # print("å­—ç¬¦ä¸²å­—ç¬¦èåˆå‘é‡:", fused_char_repr)
    return fused_char_repr

"***************************************RNAå¤„ç†************************************************"

def load_rna_3d_data(seq_file, label_file, max_len=256,stride=128):
    """
    ä»Stanford RNAæ•°æ®é›†ä¸­åŠ è½½RNAåºåˆ—å’Œå¯¹åº”3Dåæ ‡ï¼Œæ”¯æŒmaskå¤„ç†NaNç©ºå€¼ã€‚
    è¿”å›(x, y, full_ids, mask)ï¼Œç”¨äºåç»­losså±è”½ã€‚
    """
    seq_df = pd.read_csv(seq_file)
    label_df = pd.read_csv(label_file)

    data = []
    for _, row in seq_df.iterrows():
        rna_id = row['target_id']
        sequence = row['sequence']
        seq_len = len(sequence)
        # åˆå§‹åŒ–å…¨NaNçš„åæ ‡æ•°ç»„
        coords = np.full((seq_len, 3), np.nan)
         # æå– label ä¸­å±äºå½“å‰ RNA çš„åæ ‡
        for _, coord_row in label_df[label_df['ID'].str.startswith(rna_id + "_")].iterrows():
            idx = int(coord_row['ID'].split("_")[-1])
            if 0<= idx < seq_len:
                coords[idx] = [coord_row['x_1'], coord_row['y_1'], coord_row['z_1']]
        
        # ç”Ÿæˆæœ‰æ•ˆåæ ‡çš„ mask
        mask = ~np.isnan(coords).any(axis=-1)
        # å¯¹æœ‰æ•ˆåæ ‡è¿›è¡Œæ ‡å‡†åŒ–
        coords[mask] = (coords[mask] - COORDS_MEAN.cpu().numpy()) / COORDS_STD.cpu().numpy()

        if seq_len != len(coords):
            continue  # æ•°æ®å¼‚å¸¸ï¼Œè·³è¿‡

        full_ids = torch.tensor([stoi.get(c, 0) for c in sequence], dtype=torch.long)

        # å¤„ç†æ•´æ¡åºåˆ—ï¼ˆæ— æ»‘çª—ï¼‰
        if max_len is None or stride is None:
            ids = [stoi.get(c, 0) for c in sequence]
            x = torch.tensor(ids, dtype=torch.long)
            K = 5  # è¦å’Œæ¨¡å‹ä¸­çš„ K å¯¹åº”
            coord_slice = np.repeat(coords[:, np.newaxis, :], K, axis=1)  # [T, 1, 3] -> [T, K, 3]
            y = torch.tensor(coord_slice, dtype=torch.float)
            mask_tensor = torch.tensor(mask, dtype=torch.bool)
            data.append((x, y, full_ids, mask_tensor))
            continue

        # æ»‘åŠ¨çª—å£å¤„ç†
        for start in range(0, seq_len, stride):
            
            end = start + max_len
            if start >= seq_len:
                break

            seq_slice = sequence[start:end]
            coord_slice = coords[start:end]
            mask_slice = mask[start:end]

            ids = [stoi.get(c, 0) for c in seq_slice]

            # padding åˆ° max_len
            pad_len = max_len - len(ids)
            if pad_len > 0:
                ids += [0] * pad_len
                coord_slice = np.vstack([coord_slice, np.full((pad_len, 3), np.nan)])
                mask_slice = np.concatenate([mask_slice, np.zeros(pad_len, dtype=bool)])
            else:
                ids = ids[:max_len]
                coord_slice = coord_slice[:max_len]
                mask_slice = mask_slice[:max_len]

            x = torch.tensor(ids, dtype=torch.long)
            coord_slice = np.repeat(coord_slice[:, np.newaxis, :], 5, axis=1)  # [T, 1, 3] -> [T, K, 3]
            y = torch.tensor(coord_slice, dtype=torch.float)
            mask_tensor = torch.tensor(mask_slice, dtype=torch.bool)
            data.append((x, y, full_ids, mask_tensor))

    return data

class RNACoordsDataset(Dataset):
    def __init__(self, data):
        self.data = []
        for x, y, full_id, mask in data:
            if mask.sum() > 0:  # åªä¿ç•™æœ‰æ•ˆçš„æ ·æœ¬
                self.data.append((x, y, full_id, mask))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


def collate_fn(batch):
    xs, ys, full_ids,masks = zip(*batch)
    xs = pad_sequence(xs, batch_first=True, padding_value=0)
    ys = pad_sequence(ys, batch_first=True, padding_value=0.0)
    full_ids = pad_sequence(full_ids, batch_first=True, padding_value=0)
    masks = pad_sequence(masks, batch_first=True, padding_value=0.0)
    return xs, ys, full_ids,masks

class GlobalConditioning(nn.Module):
    """
    å°†å­—ç¬¦å‘é‡ä¸å…¨å±€å‘é‡é€šè¿‡éçº¿æ€§äº¤äº’æ–¹å¼èåˆã€‚
    æ”¯æŒä¸‰ç§æ–¹å¼ï¼šaddï¼ˆçº¿æ€§åŠ å’Œï¼‰ã€mlpï¼ˆéçº¿æ€§MLPæ˜ å°„ï¼‰ã€gatedï¼ˆé—¨æ§èåˆï¼‰ã€‚
    """
    def __init__(self, dim, mode='mlp'):
        super().__init__()
        self.mode = mode
        if mode == 'mlp':
            self.fuse = nn.Sequential(
                nn.Linear(dim * 2, dim),
                nn.ReLU(),
                nn.Linear(dim, dim)
            )
        elif mode == 'gated':
            self.gate = nn.Sequential(
                nn.Linear(dim * 2, dim),
                nn.Sigmoid()
            )
        # 'add' æ¨¡å¼ä¸éœ€è¦é¢å¤–å‚æ•°

    def forward(self, x, g):
        # x: [B, T, D], g: [B, D]
        B, T, D = x.shape
        g_expanded = g.unsqueeze(1).expand(-1, T, -1)  # [B, T, D]
        if self.mode == 'add':
            return x + g_expanded
        elif self.mode == 'mlp':
            return self.fuse(torch.cat([x, g_expanded], dim=-1))
        elif self.mode == 'gated':
            gate = self.gate(torch.cat([x, g_expanded], dim=-1))
            return gate * x + (1 - gate) * g_expanded
        else:
            raise ValueError(f"Unknown mode: {self.mode}")


#ä½¿ç”¨RNNåšé¢„æµ‹
class RNN3DPredictor(nn.Module):
    def __init__(self, input_dim, hidden_dim=256,k=5):
        super().__init__()
        self.rnn = nn.GRU(input_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(p=0.3)
        self.output_layer = nn.Linear(hidden_dim * 2, 3 * k)
        self.k = k
    def forward(self, x):
        rnn_out, _ = self.rnn(x)  # [B, T, 2H]

        rnn_out = self.dropout(rnn_out)

        out = self.output_layer(rnn_out)  # [B, T, 3*K]

        pred_coords = out.view(x.size(0), x.size(1), self.k, 3)  # [B, T, K, 3]
        return pred_coords

def tm_score_loss(pred, target, mask):
    B, T, K, D = pred.shape

    mask = mask.unsqueeze(-1).expand(-1, -1, K)  # [B, T, K]

    squared_dists = torch.sum((pred - target) ** 2, dim=-1) + 1e-8  # [B, T, K]
    dists = torch.sqrt(torch.clamp(squared_dists, min=1e-8))        # [B, T, K]

    valid_lengths = mask[:, :, 0].sum(dim=1)  # [B]

    # å¦‚æœvalid_lengthså¤ªå°ï¼Œå¼ºåˆ¶è°ƒæˆè‡³å°‘1
    adjusted_len = torch.clamp(valid_lengths - 15, min=1e-6)

    d0 = 1.24 * adjusted_len ** (1/3) - 1.8  # [B]
    d0 = d0.clamp(min=0.5)                   # é˜²æ­¢è´Ÿæ•°ï¼

    d0 = d0.view(B, 1, 1)                    # [B,1,1]

    tm_score_components = 1 / (1 + (dists / d0) ** 2)  # [B, T, K]
    tm_score_components = tm_score_components * mask  # maskæ— æ•ˆç‚¹

    valid_counts = mask.sum(dim=1).clamp(min=1.0)  # [B,K]

    tm_scores = tm_score_components.sum(dim=1) / valid_counts  # [B,K]

    best_tm_scores = tm_scores.max(dim=1)[0]  # [B]

    # L2æ­£åˆ™åŒ–
    pred_struct = pred[:, :, 0, :]
    pred_mask = mask[:, :, 0].unsqueeze(-1)
    pred_struct = pred_struct * pred_mask
    l2_reg = torch.mean(torch.norm(pred_struct, dim=2)) * 0.001

    return -best_tm_scores.mean() + l2_reg


def mixed_tm_l2_loss(pred, target, mask, alpha=1.0):

    # è°ƒæ•´å½¢çŠ¶: ç»Ÿä¸€ä¸º [B, K, T, 3]
    pred = pred.permute(0, 2, 1, 3)     # [B, K, T, 3]
    target = target.permute(0, 2, 1, 3) # [B, K, T, 3]
    mask = mask.unsqueeze(1).unsqueeze(-1).expand(-1, pred.size(1), -1, 1)  # [B, K, T, 1]

    pred = pred * mask
    target = target * mask

    B, K, T, _ = pred.shape

    all_tm_scores = []
    for k in range(K):
        pred_struct = pred[:, k, :, :]     # [B, T, 3]
        target_struct = target[:, k, :, :] # [B, T, 3]

        # Pairwise L2 distance per point
        squared_dists = torch.sum((pred_struct - target_struct)**2, dim=-1) + 1e-8  # [B, T]
        dists = torch.sqrt(torch.clamp(squared_dists, min=1e-8))                   # [B, T]

        # TM-score è®¡ç®—è§„åˆ™
        adjusted_len = max(T - 15, 1e-6)
        d0 = 1.24 * (adjusted_len ** (1/3)) - 1.8  # å¸¸è§„ TM-score é•¿åº¦è°ƒèŠ‚å› å­

        tm_score_components = 1 / (1 + (dists / d0)**2)  # [B, T]

        tm_score = tm_score_components.mean(dim=1)  # [B]
        all_tm_scores.append(tm_score)

    # æ‰€æœ‰Kç§é¢„æµ‹ä¸­å–æœ€ä½³TM-scoreï¼ˆæ›´ç¨³å®šï¼‰
    all_tm_scores = torch.stack(all_tm_scores, dim=1)  # [B, K]
    best_tm_scores = all_tm_scores.max(dim=1)[0]       # [B]

    # L2 Lossï¼šæˆ‘ä»¬åªå¯¹ pred[:, 0] å’Œ target[:, 0] åš MSE
    pred_l2 = pred[:, 0, :, :]     # [B, T, 3]
    target_l2 = target[:, 0, :, :] # [B, T, 3]
    l2_loss = F.mse_loss(pred_l2, target_l2)

    # å° L2 æ­£åˆ™ï¼ˆå¯é€‰ï¼‰
    l2_reg = torch.mean(torch.norm(pred_l2, dim=2)) * 0.001

    # ç»¼åˆ loss
    return -best_tm_scores.mean() + l2_reg + alpha * l2_loss


class EarlyStopping:
    def __init__(self, patience=10):
        self.patience = patience
        self.best_loss = float('inf')
        self.counter = 0

    def step(self, current_loss):
        if current_loss < self.best_loss:
            self.best_loss = current_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            if self.counter >= self.patience:
                print("â¹ï¸ Early stopping triggered.")
                return True
            return False


def RNA_3D_Predictor_Train(k_folds=5):
    # è®­ç»ƒé›†ï¼šæ»‘åŠ¨çª—å£åˆ‡ç‰‡
    train_data = load_rna_3d_data(TRAIN_SEQ_FILE_PATH, TRAIN_LABEL_FILE_PATH, max_len=256, stride=128)
    # éªŒè¯é›†ï¼šå…¨åºåˆ—
    val_data = load_rna_3d_data(VALI_SEQ_FILE_PATH, VALI_LABEL_FILE_PATH, max_len=None, stride=None)
    train_dataset = RNACoordsDataset(train_data)
    val_dataset = RNACoordsDataset(val_data)
    # train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)
    # val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)
     # åˆå§‹åŒ– KFold
    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_dataset)):
        print(f"===== Fold {fold + 1}/{k_folds} =====")
         # åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_subset = torch.utils.data.Subset(train_dataset, train_idx)
        val_subset = torch.utils.data.Subset(train_dataset, val_idx)
        train_loader = DataLoader(train_subset, batch_size=8, shuffle=True, collate_fn=collate_fn)
        val_loader = DataLoader(val_subset, batch_size=8, collate_fn=collate_fn)
    
        model = SimpleCharTransformerWithRoPE(vocab_size).to(device)
        model.load_state_dict(torch.load(RNA_TRANS_MODEL_FILE, map_location=device))
        model.eval()
        # é¢„æå–æ¯æ¡RNAåºåˆ—çš„å…¨å±€è¡¨ç¤ºï¼ˆè®­ç»ƒé›†ä¸éªŒè¯é›†éƒ½è¦ï¼‰
        string_repr_cache = build_string_repr_cache(model, train_subset)
        # éªŒè¯é›†ä¹ŸåŠ å…¥ç¼“å­˜
        string_repr_cache_val = build_string_repr_cache(model, val_subset, cache_path=RNA_PERP_CACHE_VAL_FILE)
        string_repr_cache.update(string_repr_cache_val)

        rnn_model = RNN3DPredictor(input_dim=512,k=5).to(device)
        conditioning = GlobalConditioning(dim=512, mode='gated').to(device)
        optimizer = optim.AdamW(rnn_model.parameters(), lr=1e-3)
        early_stopping = EarlyStopping(patience=5)
        for epoch in range(100):
            rnn_model.train()
            total_loss = 0
                
            for x, y, full_ids,mask in tqdm(train_loader, desc=f"Fold {fold + 1}, Epoch {epoch+1} [Train]"):
                x, y,full_ids,mask = x.to(device), y.to(device),full_ids.to(device),mask.to(device)
            
                with torch.no_grad():
                    keys = [''.join([itos[idx.item()] for idx in ids if idx.item() != 0]) for ids in full_ids]
                    full_string_repr = torch.stack([string_repr_cache[k] for k in keys]).to(device)
                    char_emb = model.embedding(x)
                    fused = conditioning(char_emb, full_string_repr)
               
               
                pred_coords = rnn_model(fused)     
                loss = tm_score_loss(pred_coords, y,mask)
                 # è°ƒè¯•ä¿¡æ¯
                # print(f"Pred coords min: {pred_coords.min().item()}, max: {pred_coords.max().item()}")
                # print(f"Target coords min: {y.min().item()}, max: {y.max().item()}")
                # print(f"Mask shape: {mask.shape}, Mask sum: {mask.sum().item()}")
                print(f"Loss: {loss.item()}")
            #     # è®¡ç®— lossï¼ˆå±è”½æ‰ mask ä¸­ä¸º False çš„ä½ç½®ï¼‰
            #     # å¹³æ–¹å·®
            #     loss = ((pred_coords - y) ** 2).sum(dim=-1)  # [B, T, K]
            #     # åº”ç”¨ maskï¼Œæ‰©å±•ç»´åº¦åŒ¹é… K
            #     loss = loss * mask.unsqueeze(-1)  # [B, T, K]
            #   # æ€»æŸå¤±é™¤ä»¥æœ‰æ•ˆåæ ‡æ•°ï¼ˆé˜²æ­¢NaNå¹²æ‰°ï¼‰
            #     loss = loss.sum() / mask.sum()
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), max_norm=1.0)
                optimizer.step()
                total_loss += loss.item()
            print(f"  Epoch {epoch+1}, Train Loss: {total_loss / len(train_loader):.4f}")
            
            # éªŒè¯é›†è¯„ä¼°
            rnn_model.eval()
            val_loss = 0
            with torch.no_grad():
                # for x, y, full_ids in val_loader:
                for x, y, full_ids,mask in tqdm(val_loader, desc=f"Fold {fold + 1},Epoch {epoch+1} [Val]"):
                    x, y, full_ids,mask = x.to(device), y.to(device), full_ids.to(device),mask.to(device)
                    keys = [''.join([itos[idx.item()] for idx in ids if idx.item() != 0]) for ids in full_ids]
                    full_string_repr = torch.stack([string_repr_cache[k] for k in keys]).to(device)
                    char_emb = model.embedding(x)
                    fused = conditioning(char_emb, full_string_repr)
                    pred_coords = rnn_model(fused)
                    loss = tm_score_loss(pred_coords,y,mask)
                    val_loss += loss.item()
                val_loss = val_loss / len(val_loader)
                print(f"  Fold {fold + 1}, Epoch {epoch + 1}, Val Loss: {val_loss:.4f}")
                if early_stopping.step(val_loss):
                    torch.save(rnn_model.state_dict(), RNA_RNN_MODEL_FILE)
                    print("âœ… æå‰åœæ­¢ï¼Œæœ€ä½³ RNN æ¨¡å‹å·²ä¿å­˜ä¸º rna_3d_rnn_model.pth")
                    break
            print(f"===== Fold {fold + 1} å®Œæˆ =====")
        print("ğŸ‰ K æŠ˜äº¤å‰éªŒè¯å®Œæˆï¼")


def build_string_repr_cache(model, dataset, cache_path=RNA_REPR_CACHE_FILE):
    if os.path.exists(cache_path):
        print(f"åŠ è½½ç¼“å­˜çš„ string_repr æ–‡ä»¶: {cache_path}")
        return torch.load(cache_path)

    print("ç”Ÿæˆ string_repr ç¼“å­˜...")
    cache = {}
    model.eval()
    with torch.no_grad():
        for _, _, full_ids,_ in dataset:
            key = ''.join([itos[idx.item()] for idx in full_ids if idx.item() != 0])
            if key in cache:
                continue
            full_ids_tensor = full_ids.unsqueeze(0).to(device)
            _ = model(full_ids_tensor)
            hidden = model.hidden_states[-1]
            mask = (full_ids_tensor != 0).unsqueeze(-1).float()
            string_repr = (hidden * mask).sum(dim=1) / mask.sum(dim=1)
            cache[key] = string_repr.squeeze(0).detach().cpu()
    torch.save(cache, cache_path)
    print(f"ç¼“å­˜å·²ä¿å­˜åˆ°: {cache_path}")
    return cache

def infer_with_sliding_window(model, x, full_string_repr, conditioning, rnn_model,
                               window_size=256, stride=128, k=5):
    """
    ä½¿ç”¨æ»‘åŠ¨çª—å£æ¨ç†å¹¶åˆå¹¶ç»“æœï¼Œè¿”å›åæ ‡å‡†åŒ–åæ ‡å’Œæœ‰æ•ˆ maskã€‚
    """
    B, T = x.shape
    device = x.device

    pred_coords_accum = torch.zeros(B, T, k, 3, device=device)
    count_accum = torch.zeros(B, T, 1, device=device)

    if T <= window_size:
        # åºåˆ—é•¿åº¦æ¯”çª—å£å°ï¼Œç›´æ¥æ•´æ®µå¤„ç†
        with torch.no_grad():
            fused = conditioning(model.embedding(x), full_string_repr)
            pred_coords = rnn_model(fused)
            pred_coords = pred_coords * COORDS_STD + COORDS_MEAN
        valid_mask = torch.ones(B, T, dtype=torch.bool, device=device)
        return pred_coords, valid_mask

    for start in range(0, T, stride):
        end = min(start + window_size, T)
        x_window = x[:, start:end]  # [B, cur_len]
        cur_len = end - start
        if cur_len == 0:
            continue

        with torch.no_grad():
            fused = conditioning(model.embedding(x_window), full_string_repr)
            pred_coords = rnn_model(fused)  # [B, cur_len, K, 3]

        # ç´¯åŠ åˆ°æ€»åæ ‡å¼ é‡ä¸­
        pred_coords_accum[:, start:end] += pred_coords
        count_accum[:, start:end] += 1

    # é¿å…é™¤0
    count_accum[count_accum == 0] = 1.0
    pred_coords_s = pred_coords_accum / count_accum

    # åæ ‡å‡†åŒ–
    pred_coords_s = pred_coords_s * COORDS_STD + COORDS_MEAN

    # æœ‰æ•ˆä½ç½® mask
    valid_mask = (count_accum.squeeze(-1) > 0)

    return pred_coords_s, valid_mask

def rna_3D_eval(input_str,k=5):
    model = SimpleCharTransformerWithRoPE(vocab_size).to(device)
    model.load_state_dict(torch.load(RNA_TRANS_MODEL_FILE, map_location=device,weights_only=True))
    model.eval()  

    input_tensor = encode_string(input_str, stoi).to(device)

    with torch.no_grad():
        logits = model(input_tensor)
    char_emb = model.embedding(input_tensor)
    last_hidden = model.hidden_states[-1]
    mask = (input_tensor != 0).unsqueeze(-1).float()
    valid_lengths = mask.sum(dim=1)  
    string_repr = (last_hidden * mask).sum(dim=1) / valid_lengths
    string_repr_expanded = string_repr.unsqueeze(1).expand_as(char_emb)
    fused_char_repr = char_emb + string_repr_expanded

    # åˆå§‹åŒ– conditioning å’Œ rnn_model
    conditioning = GlobalConditioning(dim=512, mode='gated').to(device)
    rnn_model = RNN3DPredictor(input_dim=512, k=k).to(device)
    if os.path.exists("rna_3d_rnn_model.pth"):
        rnn_model.load_state_dict(torch.load(RNA_RNN_MODEL_FILE, map_location=device,weights_only=True))
    rnn_model.eval()

    pred_coords,valid_mask = infer_with_sliding_window(model, fused_char_repr, string_repr, conditioning, rnn_model, window_size=256, stride=128, k=k)
    return pred_coords,valid_mask

def generate_submission_file(model, test_seq_file, output_path, k=5):
    test_df = pd.read_csv(test_seq_file)
    rnn_model = RNN3DPredictor(input_dim=512, k=k).to(device)
    if os.path.exists("rna_3d_rnn_model.pth"):
        rnn_model.load_state_dict(torch.load(RNA_RNN_MODEL_FILE, map_location=device,weights_only=True))
    rnn_model.eval()
    conditioning = GlobalConditioning(dim=512, mode='gated').to(device)

    model.eval()
    results = []
    for _, row in test_df.iterrows():
        rna_id = row['target_id']
        seq = row['sequence']
        input_tensor = encode_string(seq, stoi).to(device)
        with torch.no_grad():
            _ = model(input_tensor)
            char_emb = model.embedding(input_tensor)
            last_hidden = model.hidden_states[-1]
            mask = (input_tensor != 0).unsqueeze(-1).float()
            valid_lengths = mask.sum(dim=1)
            string_repr = (last_hidden * mask).sum(dim=1) / valid_lengths
            # fused = conditioning(char_emb, string_repr)
            pred_coords,valid_mask = infer_with_sliding_window(model, input_tensor, string_repr, conditioning, rnn_model, k=k)
        
        print(f"{rna_id} é¢„æµ‹åæ ‡èŒƒå›´:", pred_coords.min().item(), pred_coords.max().item())
        pred_coords = pred_coords.squeeze(0).cpu().numpy()  # [T, K, 3]
        valid_mask = valid_mask.squeeze(0).cpu().numpy()    # [T]
        for i, base in enumerate(seq):
            if not valid_mask[i]:
                continue
            coords = pred_coords[i]  # [K, 3]
            row_data = [f"{rna_id}_{i+1}", base, i+1]
            for xyz in coords:
                row_data.extend(xyz.tolist())
            results.append(row_data)

    columns = ['ID', 'resname', 'resid']
    for i in range(1, k + 1):
        columns += [f'x_{i}', f'y_{i}', f'z_{i}']
    df = pd.DataFrame(results, columns=columns)
    df.to_csv(output_path, index=False)
    print(f"âœ… Submission file saved to {output_path}")



if __name__ == "__main__":
    RNA_3D_Predictor_Train()
    # è®­ç»ƒå®Œæˆåç”Ÿæˆæäº¤æ–‡ä»¶
    generate_submission_file(
        model=SimpleCharTransformerWithRoPE(vocab_size).to(device),
        test_seq_file=TEST_SEQ_FILE_PATH,
        output_path="submission.csv",
        k=5
    )

